{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# O que é machine learning?\n",
    "\n",
    "Machine learning, ou aprendizado de máquinas, é o desenvolvimento de algoritmos genéricos que conseguem identificar e aprender coisas sobre um determinado monte de dados sem ser explicitamente programado para tal.\n",
    "\n",
    "Ao invés de escrever decisões lógicas, você alimenta seu algorítmo com dados e ele decide como interpretar aquilo e tomar decisões em cima dos dados.\n",
    "\n",
    "Por exemplo, você pode alimentar um algoritmo com dados sobre números escritos à mão e pedir que ele identifique novos números. Você pode, também, alimentar esse mesmo algoritmo com dados sobre emails que são spam e não spam e pedir que ele identifique novos emails.\n",
    "\n",
    "![algorimtos são caixas pretas](../static/images/ml_blackbox.png)\n",
    "\n",
    "# Tipos de machine learning\n",
    "Machine learning é um campo vasto. As 3 sub-áreas mais comuns são, porém, **aprendizado supervisionado**, **não supervisionado** e **de reforço**.\n",
    "\n",
    "Nessa primeira aula vamos focar no primeiro tipo. Na próxima, falaremos de técnicas de otimização e do segundo. O terceiro tipo está fora do escopo dessas aulas.\n",
    "\n",
    "Esses 3 tipo são definidos mais ou menos assim:\n",
    "\n",
    "## Aprendizado supervisionado\n",
    "Para ilustrar o conceito, vamos pensar em um exemplo.\n",
    "\n",
    "Digamos que estamos nos anos 90 e você trabalha em uma locadora de filmes. Sua locadora é diferenciada porque vocês costumam recomendar filmes para os seus clientes. Contudo, você percebe que tem um grande problema quando vocês contratam alguém novo. Esse novo empregado não conhece todos os filmes da sua locadora e, por isso, não consegue recomendar muitos filmes para os clientes. Você descobre que vocês têm uma tabela com várias informações como gênero, ano, duração e vários outras características dos filmes. Você também sabe que têm uma tabela com os últimos filmes que todos os seus clientes alugaram.\n",
    "\n",
    "Você decide então escrever um programa que tenta recomendar filmes para os seus clientes com base no que eles costumam assistir.\n",
    "\n",
    "![filmes](../static/images/movies.png)\n",
    "\n",
    "\n",
    "Vocês então recebem um filme novo, Rei leão, e devem decidir quem seria o cliente que mais gostaria daquele filme:\n",
    "\n",
    "\n",
    "![cliente](../static/images/client.png)\n",
    "\n",
    "Isso é aprendizado supervisionado. Você entendeu o padrão e foi capaz de encontrar qual a pessoa que poderia gostar mais desse filme. Nesse caso, talvez o ano ou nome não façam muita diferença, mas em casos mais complexos, como a previsão de preços de casas ou de músicas, seja necessário mais cálculos.\n",
    "\n",
    "O seu algoritmo tem que ser capaz de ir além desses dados e generalizar as características para decidir.\n",
    "\n",
    "Em aprendizado supervisionado você deixa o computador descobrir essa lógica para você e, com ela, ele é capaz de fazer previsões e trabalhar com dados novos.\n",
    "\n",
    "## Aprendizado não supervisionado\n",
    "\n",
    "Em aprendizado supervisionado nós não temos o que cada filme **é**. Temos apenas dados e temos que extrair informações deles. Em nosso exemplo anterior, nós podemos aplicar algoritmos de aprendizado não supervisionado para identificar mercados nos filmes. Podemos usar as idades dos clientes para descobrir que gêneros de filmes são mais populares com que faixas etárias, descobrir se homens gostam mais de filmes de ação que mulheres e até fazer análises ao longo do tempo pra saber exatamente onde e como devemos promover um filme para obter o máximo possível de clientes interessados.\n",
    "\n",
    "## Aprendizado de reforço\n",
    "\n",
    "Aprendizado de reforço é muito menos sobre previsões e muito mais sobre resultados. Usamos aprendizado de reforço quando queremos criar algoritmos novos que atingem o melhor resultado possível.\n",
    "\n",
    "Digamos que queremos manter um cliente sendo fiel durante muito tempo. Para isso nós mandamos emails de recomendação e enviamos presentes de vez em quando para manter ele interessado em nós. Porém, não queremos que ele sinta que estamos forçando a barra. Como encontrar esse equilibrio?\n",
    "\n",
    "Primeiro criamos um algoritmo que decide quando enviar um email ou um presente. A partir daí, começamos a mandar emails e presentes e avaliar a resposta dos clientes. Sempre que um cliente sai da nossa lista de email nós dizemos pro algoritmo que ele fez besteira. Sempre que ele recomenda o nosso sistema para alguém, dizemos que ele fez bem. Ao longo do tempo ele começa a aprender quais padrões de presentes e emails rendem mais \"elogios\" e menos \"críticas\". Nós usamos essas reflexões para **reforçar comportamentos** que são bons ou ruins e incentivamos os algoritmos a se melhorarem e se adaptarem. Isso é diferente de um algoritmo supervisionado pois o feedback pode demorar meses para vir e o algoritmo tem que ser capaz de refletir sobre suas ações e experimentar coisas novas para **otimizar** seus resultados.\n",
    "\n",
    "\n",
    "Agora que já vimos um pouco sobre eles, vamos mais a fundo em algortimos supervisionados.\n",
    "\n",
    "# Introdução ao aprendizado supervisionado\n",
    "\n",
    "Ok, já vimos a teoria, agora vamos escrever um programa que encontre filmes para a gente!\n",
    "\n",
    "Se você não soubesse nada sobre aprendizado de máquinas, talvez escrevesse um programa mais ou menos assim:\n",
    "\n",
    "```Python\n",
    "def recomendar_filme(filme, ano, gênero, lista_de_filmes):\n",
    "    \n",
    "    # Encontra os filmes que tem o mesmo genero do filme novo\n",
    "    filmes_com_mesmo_genero = filter(lambda x: x['gênero'] == gênero, lista_de_filmes)\n",
    "    \n",
    "    # Dentro desses, acha os que tem o um ano próximo\n",
    "    filmes_ano_proximo = filter(lambda x: ano + 5 >= x['ano'] >= ano - 5, filmes_com_mesmo_genero)\n",
    "    \n",
    "    # Desses encontrados, pegar o cliente que alugou um desses filmes e recomendar pra ele\n",
    "    cliente = filmes_ano_proximo[0]['cliente']\n",
    "    return cliente\n",
    "```\n",
    "\n",
    "Se você ficar mexendo bastante nisso por horas talvez consiga alguma coisa que funcione, mas seu programa nunca será perfeito e sempre terá que alterar alguma coisa pra filmes novos.\n",
    "\n",
    "O ideal seria simplesmente pedir que o computador fizesse tudo pra você\n",
    "\n",
    "```Python\n",
    "def recomendar_filme(filme, ano, gênero, lista_de_filmes):\n",
    "    cliente = <computador, por favor, faz esses cálculos por mim>\n",
    "    \n",
    "    return cliente\n",
    "```\n",
    "\n",
    "O que ele vai fazer na verdade é transformar esses valores que nós criamos, gêneros, anos e tal em **vetores em um hiperespaço** que pode ser representado por números.\n",
    "\n",
    "No fim o que o computador vai gerar é algo como:\n",
    "\n",
    "```Python\n",
    "def recomendar_filme(filme, ano, gênero, lista_de_filmes):\n",
    "    vetor_gênero = *vetorizador_gênero(gênero) * [63.61124, .053823, 23.23521, 294.43213, .000123, 1.]\n",
    "    \n",
    "    vetor_ano = *vetorizador_ano(ano) * [54.2421543, .00342, 5143532.3415]\n",
    "    \n",
    "    vetor_cliente = 873.5323 + vetor_gênero + vetor_ano\n",
    "    \n",
    "    cliente = distribuição_prob(lista_de_filmes, vetor_cliente)\n",
    "    \n",
    "    return cliente\n",
    "```\n",
    "\n",
    "Todos esses valores nas listas são o que chamamos de **pesos**. Eles são gerados **aleatoriamente** pelo computador e testados milhares de vezes até que o valor que obtem o **menor erro** seja encontrado. Como podem ver, são vetores multi dimensionais que podem ter, na verdade, centenas de dimensões. Cada dimensão representa um ano ou gênero de filme e os pesos representam a interação entre eles.\n",
    "\n",
    "No fim nós podemos usar uma distribuição probabilística para encontrar o cliente que teria a maior probabilidade de gostar do filme que estamos analisando.\n",
    "\n",
    "É imporante ressaltar que os valores iniciais desses pesos são aleatórios. Para encontrar os valores finais, existem diversas técnicas de otimização que, eventualmente, tendem a reduzir o erro total do sistema a um mínimo.\n",
    "\n",
    "Para calcular esse erro, podemos usar uma função de **custo** que define a **distância** entre o valor previsto e o valor definido como correto (aí está, novamente, por que estamos usando aprendizado supervisionado).\n",
    "\n",
    "A **função de custo** pode tomar diversas formas, mas a mais comum e talvez a mais simples é a **soma do erro quadrado**, onde por erro nós definimos a distância probabilística até o valor correto.\n",
    "\n",
    "\\begin{align}\n",
    "Custo = \\frac{\\sum_{i=1}^n \\left( meu\\_chute(i) - valor\\_real(i) \\right)^2}{2n}\n",
    "\\end{align}\n",
    "\n",
    "Vamos agora reescrever essa função usando alguns jargões de machine learning que você não tem que se preocupar agora\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=0}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "\\end{align}\n",
    "\n",
    "Onde $\\theta$ representa os **pesos** e $J(\\theta)$ o **custo para os pesos atuais**\n",
    "\n",
    "Se nós fizermos um gráfico dos pesos, teremos algo assim:\n",
    "\n",
    "![pesos](../static/images/pesos.png)\n",
    "\n",
    "Onde o eixo vertical representa o valor do custo dados os pesos nos eixos horizontais.\n",
    "\n",
    "Nesse gráfico, o que queremos fazer é reduzir ao mínimo o nosso custo (erro). Para isso usamos técnicas de otimização, onde fazemos algo assim:\n",
    "\n",
    "![pesos](../static/images/pesos2.png)\n",
    "\n",
    "\n",
    "Para isso nós precisamos ajustar nossos pesos de forma que estejamos 'descendo a ladeira' em direção ao mínimo.\n",
    "\n",
    "Ter o erro mínimo significa estar o mais próximo possível, teoreticamente, da previsão perfeita.\n",
    "\n",
    "Se você lembra algo de cálculo, deve saber que se nós pegarmos a derivada de uma função em cada variável e somarmos, obtemos uma outra função cujo valor define a direção de crescimento máximo. ([gradiente](https://en.wikipedia.org/wiki/Gradient))\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla f(x, y) = \\frac{\\partial f}{\\partial x} i + \\frac{\\partial f}{\\partial y} j\n",
    "\\end{align}\n",
    "\n",
    "Esse é um método de otimização simples, mas eficiente, que nos permite rapidamente encontrar os valores de peso mínimo de nossa fução de custo e, logo, o **estado mais otimizado do nosso modelo**.\n",
    "\n",
    "Isso é apenas uma introdução, então não vamos ir mais fundo que isso, mas sinta-se livre para [aprender mais caso queira](http://hbfs.wordpress.com/2012/04/24/introduction-to-gradient-descent/).\n",
    "\n",
    "Não se preocupe, porém, em ter tudo isso na cabeça. Quando você usar uma biblioteca de machine learning tudo isso será **feito pela biblioteca, não por você**. Porém é importante ter uma noção do que está acontecendo.\n",
    "\n",
    "## O que acabamos de aprender?\n",
    "\n",
    "Isso tudo que nós vimos é o funcionamento de uma **regressão linear multivariacional**. Vamos ver isso novamente mais pra frente do curso, incluindo alguns conceitos como **overfitting**, **tendência, ou bias**, **validação** e outras coisas.\n",
    "\n",
    "Saiba, porém, que nem sempre será possível usar esses algoritmos para prever essas coisas. Isso acontece porque muitas vezes as relações entre essa variáveis (ano/gênero) não são lineares. Para esses outros casos nós teremos que usar outros **modelos** e até outras técnicas. Veremos tudo isso mais pra frente.\n",
    "\n",
    "## Aprendizado de máquinas é mágica?\n",
    "Assim que nós começarmos a aplicar machine learning para solucionar alguns problemas, começaremos a perceber que é muito fácil solucionar problemas que parecem ser muito difíceis de se resolver de outra forma.\n",
    "\n",
    "É importante notar, porém, que aprendizado de máquinas só funciona para resolver problemas que **podem ser resolvidos**.\n",
    "\n",
    "Por exemplo, não é possível determinar o tipo de filme que alguém gosta baseado na raça de cachorro que essa pessoa tem. Não importa o quanto você altere seu algoritmo e o quão bons os seus resultados fiquem para o dataset de treino, será impossível ter uma boa validação fora disso. Seu modelo sofrerá de overfitting e você precisará de outras características para resolver esse problema.\n",
    "\n",
    "![magica](../static/images/magicas.png)\n",
    "\n",
    "\n",
    "Com isso nós concluímos nossa introdução ao ML. Agora vamos começar a programar algumas coisas e fazer ML de verdade!\n",
    "\n",
    "# Programando Machine Learning\n",
    "\n",
    "Para programar e usar machine learning, vamos usar uma biblioteca de ML muito famosa chamada SKLearn. Ela contém centenas de modelos e algoritmos de classificação, regressão, clusterização e muito mais.\n",
    "\n",
    "Você pode ler sobre o projeto no site deles:\n",
    "[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)\n",
    "\n",
    "![scikit](../static/images/scikit.png)\n",
    "\n",
    "\n",
    "## Algoritmos e superficies de decisão\n",
    "\n",
    "Geralmente nós conseguimos plotar nossos dados em gráficos, onde cada eixo representa uma característica dos dados e cada ponto um dado específico. Geralmente as cores representam o tipo, ou **label** de cada dado.\n",
    "\n",
    "Primeiro vamos definir uma função que gera gráficos para nós. Isso vai ser bem útil no futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(blue, red, test_size=0.2, func=lambda x: x):\n",
    "    \n",
    "    blue_x = np.vectorize(func)(np.random.randint(*blue[0]))\n",
    "    blue_y = np.vectorize(func)(np.random.randint(*blue[1]))\n",
    "    red_x = np.vectorize(func)(np.random.randint(*red[0]))\n",
    "    red_y = np.vectorize(func)(np.random.randint(*red[1]))\n",
    "    \n",
    "    blue_vals = np.dstack((blue_x, blue_y, [\"blue\" for _ in blue_x]))[0]\n",
    "    red_vals = np.dstack((red_x, red_y, [\"red\" for _ in red_x]))[0]\n",
    "    \n",
    "    X = np.append(blue_vals, red_vals, axis=0)\n",
    "    \n",
    "    Y = [1 for _ in range(len(blue_vals))]\n",
    "    Y.extend([0 for _ in range(len(red_vals))])\n",
    "    \n",
    "    return train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_data([[0, 40, 100], [0, 40, 100]], [[50, 100, 100], [50, 100, 100]])\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ver, nós temos dados separados no que são claramente objetos diferentes. É muito fácil criar uma barreira que separa os dois clusters.\n",
    "\n",
    "Depois de separar, fica bem fácil assumir que tudo que estiver de um lado dela será azul e do outro vermelho. Essa barreira se chama uma **superfície de decisão** e é um dos conceitos principais quando lidamos com algoritmos de classificação.\n",
    "\n",
    "Agora vamos ver um algoritmo de classificação e experimentar com ele.\n",
    "\n",
    "# Naive Bayes\n",
    "Para o nosso primeiro modelo de aprendizado supervisionado, aprenderemos um modelo de classificação probabilístico chamado Naive Bayes, que vem de um homem religioso chamado Bayes que criou o teorema de Bayes para provar a existência de Deus.\n",
    "\n",
    "O algoritmo, como o nome sugere, usa o [Teorema de Bayes](https://en.wikipedia.org/wiki/Bayes%27_theorem) para descobrir a probabilidade de um novo elemento se encaixar numa categoria específica.\n",
    "\n",
    "\n",
    "Usando o SKLearn, nosso código sempre terá um padrão como veremos abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Em X_train nós temos os nossos dados brutos, suas coordenadas. \n",
    "# Em y_train nós temos a categoria (1 para azul e 0 para vermelho)\n",
    "# Não se preocupe com os _test por enquanto\n",
    "X_train, X_test, y_train, y_test = generate_data([[0, 40, 100], [0, 40, 100]], [[50, 100, 100], [50, 100, 100]])\n",
    "\n",
    "# Desses dados, nós podemos importar o classificador de Nayve Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Criar um objeto classficador\n",
    "clf = GaussianNB()\n",
    "\n",
    "# E fitar, ou seja, treinar nosso clssificador usando os dados de X_train e y_train\n",
    "clf.fit(X_train[:, :2].astype(np.int), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Agora nós criamos uma outra função que vai criar um gráfico e desenhar a nossa superfície de decisão\n",
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_boundary(clf, X):\n",
    "    h = .02\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].astype(np.float).min() - 1, X[:, 0].astype(np.float).max() + 1\n",
    "    y_min, y_max = X[:, 1].astype(np.float).min() - 1, X[:, 1].astype(np.float).max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    print(\"predicting\")\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print(\"predicted\")\n",
    "    print(\"length\", len(Z))\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    print(\"colormesh\")\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0].astype(np.float), X[:, 1].astype(np.float), c=X[:, 2])\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    print(\"showing\")     \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, passamos os argumentos para nossa função\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode demorar um tempo, mas no final você deverá ser capaz de ver bem onde exatamente a decisão é feita para separar ambas as cores. Qualquer ponto na zona azul será previsto por nosso algoritmo como azul e o mesmo vale para a zona vermelha.\n",
    "\n",
    "Nós podemos, por exemplo, testar nosso algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos um novo ponto que queremos testar\n",
    "novo_ponto = (19, 80)\n",
    "\n",
    "# Geramos uma previsão para ele. Se sair 1, será azul e 0 será vermelho\n",
    "print(clf.predict([novo_ponto]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excelente! Temos nosso primeiro classificador. Agora algo importante que devemos fazer é testar esse classificador.\n",
    "\n",
    "## Testando\n",
    "\n",
    "Para isso, usaremos uma ferramenta muito útil chamada **matriz de confusão**. Essa matriz nos diz exatamente quantos pontos foram corretamente previstos, quantos foram falsos positivos e quantos foram falsos negativos. Para isso, criamos uma função que gera essa matriz para nós:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_predicted):\n",
    "    plt.matshow(confusion_matrix(y_true, y_predicted), cmap=plt.cm.binary, interpolation='nearest')\n",
    "    plt.title('Matriz de previsão')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Categoria esperada')\n",
    "    plt.xlabel('Categoria prevista')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos ver como fica para os dados que usamos para treinar\n",
    "plot_confusion_matrix(y_train, clf.predict(X_train[:, :2].astype(np.int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse formato que estamos vendo é o ideal. Ele representa que todos os valores de azul foram previstos corretamente e os de vermelho também, sem falsos positivos ou negativos.\n",
    "\n",
    "Nós também podemos gerar algo chamado **relatório de classificação**, que faz exatamente o que o nome sugere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, clf.predict(X_train[:, :2].astype(np.int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse relatório, temos 4 valores:\n",
    "\n",
    "- **Precisão**: Relação entre os verdadeiros positivos e o total. Diz o quão preciso foi a classificação dado todos os dados\n",
    "- **Recall**: Relação entre verdadeiros positivos e positivos. Diz o quão certeiro foi a classificação para encontrar os positivos\n",
    "- **F1-Score**: Média harmônica entre precisão e recall. Boa forma de avaliar de forma completa a classificação\n",
    "- **Support**: É o número de ocorrências de cada classe em verdadeiro-positivo\n",
    "\n",
    "Esses dados são bacanas, mas não são muito interessantes do ponto de vista de previsões futuras. O que nós queremos acima disso é checar se nosso algoritmo é capaz de ser generalizado para dados novos.\n",
    "\n",
    "Para isso vamos usar aquelas variáveis que criamos antes: `X_test` e `y_test` que não foram fornecidas ao nosso classificador para treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.int)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ver, mesmo assim o nosso algoritmo foi fantástico na classificação. Isso é de se esperar, dado que foram dados bem simples.\n",
    "\n",
    "Vamos testar algo mais complexo agora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_data([[0, 100, 200], [0, 60, 200]], [[00, 100, 250], [20, 100, 250]])\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ver, esse já é mais difícil pois temos muita intersecção entre os pontos. Como noss algoritmo vai se sair agora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train[:, :2].astype(np.int), y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.int))))\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.int)))\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% de precisão não é tão bom assim. Dá pra ver por que foi tão ruim.\n",
    "\n",
    "Nossas features não estão sendo suficientes para separar corretamente os dados. Para separar, nós precisaremos adiconar features novas que, em outras dimensões, separem os nossos dados. Uma visualização desse resultado seria:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "sequence_containing_x_vals = list(range(0, 50))\n",
    "sequence_containing_y_vals = list(range(0, 50))\n",
    "sequence_containing_z_vals = list(range(50, 100))\n",
    "\n",
    "random.shuffle(sequence_containing_x_vals)\n",
    "random.shuffle(sequence_containing_y_vals)\n",
    "random.shuffle(sequence_containing_z_vals)\n",
    "\n",
    "ax.scatter(sequence_containing_x_vals, sequence_containing_y_vals, sequence_containing_z_vals)\n",
    "sequence_containing_x_vals = list(range(50, 100))\n",
    "sequence_containing_y_vals = list(range(50, 100))\n",
    "sequence_containing_z_vals = list(range(50, 100))\n",
    "\n",
    "random.shuffle(sequence_containing_x_vals)\n",
    "random.shuffle(sequence_containing_y_vals)\n",
    "random.shuffle(sequence_containing_z_vals)\n",
    "\n",
    "ax.scatter(sequence_containing_x_vals, sequence_containing_y_vals, sequence_containing_z_vals)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Agora vamos sair um pouco de Nayve Bayes e testar outro algoritmo de classificação. Esse agora se chama Support Vector Machines e é melhor em algumas situações em comparação com o Nayve Bayes.\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "Para treinar um SVM, fazemos da mesma forma que fazíamos com NB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_data([[0, 40, 100], [0, 40, 100]], [[50, 100, 100], [50, 100, 100]])\n",
    "\n",
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(X_train[:, :2].astype(np.int), y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.int))))\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.int)))\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podem ver, é bem parecido com o NB, mas é mais linear.\n",
    "\n",
    "O que acontece quando nossa barreira de classificação não é separavel linearmente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_circular_data(min_radius, med_radius, max_radius, d1=100, d2=100, test_size=0.2, func=lambda x: x):\n",
    "    \n",
    "    x1 = np.random.uniform(-min_radius, min_radius, d1)\n",
    "    y1 = []\n",
    "    for i in x1:\n",
    "        yy = np.random.uniform(0, sqrt(min_radius**2 - i**2))\n",
    "        if random.choice([True, False]):\n",
    "            yy = -yy\n",
    "        y1.append(yy)\n",
    "\n",
    "    x2 = np.random.uniform(-max_radius, max_radius, d2)\n",
    "    y2 = []\n",
    "    for i in x2:\n",
    "        low = sqrt(med_radius**2 - i**2) if med_radius**2 - i**2 > 0 else 0\n",
    "        yy = np.random.uniform(low, sqrt(max_radius**2 - i**2))\n",
    "        if random.choice([True, False]):\n",
    "            yy = -yy\n",
    "        y2.append(yy)\n",
    "    \n",
    "    blue_vals = np.dstack((np.vectorize(func)(x1), np.vectorize(func)(y1), [\"blue\" for _ in x1]))[0]\n",
    "    red_vals = np.dstack((np.vectorize(func)(x2), np.vectorize(func)(y2), [\"red\" for _ in x2]))[0]\n",
    "    \n",
    "    X = np.append(blue_vals, red_vals, axis=0)\n",
    "    \n",
    "    Y = [1 for _ in range(len(blue_vals))]\n",
    "    Y.extend([0 for _ in range(len(red_vals))])\n",
    "    \n",
    "    return train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_circular_data(10, 15, 20, 100, 200)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(X_train[:, :2].astype(np.float), y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.float))))\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.float)))\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É facil perceber que essa barreira será difícil pro nosso SVM classifier. O que podemos fazer então?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos criar features novas a partir das anteriores. Nesse caso, podemos criar uma feature que representa o **quadrado das variáveis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = lambda x: x**2\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_circular_data(10, 15, 20, 100, 200, func=square)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, finalmente, podemos testar nosso classificador usando essas features novas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"linear\")\n",
    "clf.fit(X_train[:, :2].astype(np.float), y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.float))))\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.float)))\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De fato, é isso que o SVC é capaz de fazer sozinho. Ao contrário do NB, o SVC é capaz de usar **truques de kernel** para explorar variáveis novas e configurações novas de variáveis. Experimentemos rodar o código para o gráfico circular, mas com a kernel sendo a padrão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_circular_data(10, 15, 20, 100, 200)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "plt.show()\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train[:, :2].astype(np.float), y_train)\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.float))))\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.float)))\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
