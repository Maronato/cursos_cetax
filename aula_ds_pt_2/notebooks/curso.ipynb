{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from math import sqrt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(blue, red, test_size=0.2, func=lambda x: x):\n",
    "    \n",
    "    blue_x = np.vectorize(func)(np.random.randint(*blue[0]))\n",
    "    blue_y = np.vectorize(func)(np.random.randint(*blue[1]))\n",
    "    red_x = np.vectorize(func)(np.random.randint(*red[0]))\n",
    "    red_y = np.vectorize(func)(np.random.randint(*red[1]))\n",
    "    \n",
    "    blue_vals = np.dstack((blue_x, blue_y, [\"blue\" for _ in blue_x]))[0]\n",
    "    red_vals = np.dstack((red_x, red_y, [\"red\" for _ in red_x]))[0]\n",
    "    \n",
    "    X = np.append(blue_vals, red_vals, axis=0)\n",
    "    \n",
    "    Y = [1 for _ in range(len(blue_vals))]\n",
    "    Y.extend([0 for _ in range(len(red_vals))])\n",
    "    \n",
    "    return train_test_split(X, Y, test_size=test_size)\n",
    "def plot_decision_boundary(clf, X):\n",
    "    h = .02\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].astype(np.float).min() - 1, X[:, 0].astype(np.float).max() + 1\n",
    "    y_min, y_max = X[:, 1].astype(np.float).min() - 1, X[:, 1].astype(np.float).max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    print(\"predicting\")\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    print(\"predicted\")\n",
    "    print(\"length\", len(Z))\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    print(\"colormesh\")\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0].astype(np.float), X[:, 1].astype(np.float), c=X[:, 2])\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    print(\"showing\")     \n",
    "    plt.show()\n",
    "def plot_confusion_matrix(y_true, y_predicted):\n",
    "    plt.matshow(confusion_matrix(y_true, y_predicted), cmap=plt.cm.binary, interpolation='nearest')\n",
    "    plt.title('Matriz de previsão')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Categoria esperada')\n",
    "    plt.xlabel('Categoria prevista')\n",
    "    plt.show()\n",
    "\n",
    "def generate_circular_data(min_radius, med_radius, max_radius, d1=100, d2=100, test_size=0.2, func=lambda x: x):\n",
    "    \n",
    "    x1 = np.random.uniform(-min_radius, min_radius, d1)\n",
    "    y1 = []\n",
    "    for i in x1:\n",
    "        yy = np.random.uniform(0, sqrt(min_radius**2 - i**2))\n",
    "        if random.choice([True, False]):\n",
    "            yy = -yy\n",
    "        y1.append(yy)\n",
    "\n",
    "    x2 = np.random.uniform(-max_radius, max_radius, d2)\n",
    "    y2 = []\n",
    "    for i in x2:\n",
    "        low = sqrt(med_radius**2 - i**2) if med_radius**2 - i**2 > 0 else 0\n",
    "        yy = np.random.uniform(low, sqrt(max_radius**2 - i**2))\n",
    "        if random.choice([True, False]):\n",
    "            yy = -yy\n",
    "        y2.append(yy)\n",
    "    \n",
    "    blue_vals = np.dstack((np.vectorize(func)(x1), np.vectorize(func)(y1), [\"blue\" for _ in x1]))[0]\n",
    "    red_vals = np.dstack((np.vectorize(func)(x2), np.vectorize(func)(y2), [\"red\" for _ in x2]))[0]\n",
    "    \n",
    "    X = np.append(blue_vals, red_vals, axis=0)\n",
    "    \n",
    "    Y = [1 for _ in range(len(blue_vals))]\n",
    "    Y.extend([0 for _ in range(len(red_vals))])\n",
    "    \n",
    "    return train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Agora vamos falar um pouco sobre outro algoritmo de classificação e regressão: **Árvores de Decisão**\n",
    "\n",
    "Esse algoritmo é bastante velho e bastante interessante. Ele é especialmente curioso em suas superficies de decisão, como veremos.\n",
    "\n",
    "Decision trees são capazes de criar superfícies de decisão não lineares usando decisões lineares\n",
    "\n",
    "Por exemplo, peguemos esse dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_data([[50, 100, 220], [0, 45, 220]], [[50, 100, 200], [50, 100, 200]])\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "X_train1, X_test1, y_train1, y_test1 = generate_data([[0, 45, 150], [45, 100, 150]], [[0, 45, 150], [0, 45, 150]])\n",
    "plt.scatter(X_train1[:, 0], X_train1[:, 1], c=X_train1[:, 2])\n",
    "plt.show()\n",
    "X_train = np.concatenate([X_train, X_train1])\n",
    "X_test = np.concatenate([X_test, X_test1])\n",
    "y_train = np.concatenate([y_train, y_train1])\n",
    "y_test = np.concatenate([y_test, y_test1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Você diria que ele é linearmente separável?\n",
    "\n",
    "Ele obviamente não é, porém é fácil ver como separar ele.\n",
    "\n",
    "Talvez se nós usássemos algo como SVMs tivéssemos resultados ruins devido à não linearidade dos dados (Experimente!)\n",
    "\n",
    "É nesse tipo de caso que Decision Trees brilham. Elas são capazes de dividir o banco de dados a partir de pontos de decisão e criar árvores que decidem, com base nos pontos, onde cortar cada coisa.\n",
    "\n",
    "Isso faz com que decision trees sejam bem rápidos e eficientes em datasets como esses.\n",
    "\n",
    "Vamos ver como o dataset fica após aplicar esse algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train[:, :2].astype(np.int), y_train)\n",
    "\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.int)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.int))))\n",
    "\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exatamente como esperávamos! Ela foi capaz de identificar os padões e traçar curvas de decisões não lineares.\n",
    "\n",
    "Vamos comparar esses resultados com outro algoritmo, como Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train[:, :2].astype(np.int), y_train)\n",
    "\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.int)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.int))))\n",
    "\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem pior, né? Dá pra ver que o naive bayes tenta separar em duas superfícies probabilisticas mas é muito difícil porque a estrutura dos dados é complexa.\n",
    "\n",
    "E o SVM? Como se sairia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train[:, :2].astype(np.int), y_train)\n",
    "\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.int)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.int))))\n",
    "\n",
    "# %time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um pouco melhor, mas ainda não chega nem perto das decision trees.\n",
    "\n",
    "Decision trees são as melhores então?\n",
    "\n",
    "Depende! Para visualizar isso, vamos usar um outro exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_circular_data(10, 15, 20, 200, 100)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso nós temos bem mais intersecção entre os dados. Eles estão bem sujos com ruidos e talvez essas features não sejam as melhores.\n",
    "\n",
    "Nesse caso, vamos ver como decision trees se saem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train[:, :2].astype(np.float), y_train)\n",
    "\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.float)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.float))))\n",
    "\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não tão bom, né? Vamos comparar com outras coisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train[:, :2].astype(np.float), y_train)\n",
    "\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.float)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.float))))\n",
    "\n",
    "%time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train[:, :2].astype(np.float), y_train)\n",
    "\n",
    "plot_confusion_matrix(y_test, clf.predict(X_test[:, :2].astype(np.float)))\n",
    "print(classification_report(y_test, clf.predict(X_test[:, :2].astype(np.float))))\n",
    "\n",
    "# %time plot_decision_boundary(clf, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso nos mostra que a escolha de qual classificador usar varia bastante do estado dos seus dados. Não é algo que você pode sempre repetir sem estudar como seus dados estão.\n",
    "\n",
    "Agora vamos falar de outros tópicos.\n",
    "\n",
    "# Acesso a bancos de dados\n",
    "\n",
    "Python conta com bibliotecas para tudo, inclusive acessos a bancos de dados. Vamos usar uma biblioteca chamada psycopg2 para acessar um banco de dados Postgres na AWS\n",
    "\n",
    "Esse é um banco de dados de textos Spotted da Unicamp. Temos um total de 2000 spotteds aprovados e 2000 spotteds rejeitados em duas tabelas\n",
    "\n",
    "- aprovados\n",
    "- reprovados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import urllib.parse as urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_db(url):\n",
    "    url = urlparse.urlparse(url)\n",
    "    dbname = url.path[1:]\n",
    "    user = url.username\n",
    "    password = url.password\n",
    "    host = url.hostname\n",
    "    port = url.port\n",
    "    psql = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port, sslmode='require')\n",
    "    return psql.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor = connect_db(\"postgres://cetax:cetax1234@cetax.cc8zbwh2kw3m.us-east-1.rds.amazonaws.com:5432/cetax_ds2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, criamos um cursor do banco de dados e agora já podemos fazer pedidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = \"\"\"                              \n",
    "SELECT column_name, data_type\n",
    "FROM information_schema.columns\n",
    "WHERE table_name = %s;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Tabela aprovados\")\n",
    "cursor.execute(q, ('aprovados', ))\n",
    "print(cursor.fetchall())\n",
    "print()\n",
    "print(\"Tabela reprovados\")\n",
    "cursor.execute(q, ('reprovados', ))\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT message FROM aprovados\")\n",
    "cursor.fetchmany(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos recuperar dados de bancos de dados, vamos usar eles para aprender algumas coisas\n",
    "\n",
    "# Aprendizado por texto\n",
    "\n",
    "A maioria da internet é feita de textos, então uma das principais areas de machine learning é a análise de textos.\n",
    "\n",
    "O problema é que computadores não sabem ler textos. Eles sabem fazer cálculos, mas são bem ruins com textos. O que fazer?\n",
    "\n",
    "Para analisar textos nós precisamos converter nossos textos em **vetores** numéricos que representam nosso texto em forma de números.\n",
    "\n",
    "A forma mais comum de se fazer isso é usando uma **Bag of Words**. Com ela nós podemos vetorizar informações e avaliar elas.\n",
    "\n",
    "Mas o que é uma bag of words. Uma bag of words é uma lista de vetores, ou um **hiperplano** que contém um valor para cada palavra além de um número de vezes que cada palavra aparece em cada texto. Vamos ver isso mais pra frente.\n",
    "\n",
    "Para começar, vamos importar o pacote de SKLearn que nos permite criar bags of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto. Agora nós podemos alimentar nossa bag of words com todas as palavras do nosso banco de dados.\n",
    "\n",
    "Mas primeiro, vamos criar um dataframe para conter esses dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT message FROM aprovados\")\n",
    "spotteds = list(map(lambda x: [x[0], 1], cursor.fetchall()))\n",
    "\n",
    "cursor.execute(\"SELECT message FROM reprovados\")\n",
    "spotteds.extend(list(map(lambda x: [x[0], 0], cursor.fetchall())))\n",
    "df = pd.DataFrame(spotteds, columns=['mensagem', 'aprovado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto, agora podemos criar um vetorizador e alimentar ele com nossos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vetorizador = CountVectorizer()\n",
    "\n",
    "vetorizador_fitado = vetorizador.fit(df['mensagem'])\n",
    "\n",
    "bag_of_words = vetorizador_fitado.transform(df['mensagem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso que nós temos agora é uma série de duplas, onde o primeiro elemento indica o texto(spotted) e o segundo elemento o índice da palavra, seguido do número de vezes que foi encontrada nesse documento\n",
    "\n",
    "Podemos fazer alguns testes, como por exemplo pegar uma palavra específica e ver quantas vezes apareceu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vetorizador.vocabulary_.get(\"amor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra coisa importante é a normalização.\n",
    "\n",
    "Tfidf é um tipo de normalizador de textos que pega o inverso da frequência por documento para dar mais valor a elementos raros que melhor podem definir nosso banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(bag_of_words)\n",
    "\n",
    "norm_bag_of_words = tfidf_transformer.transform(bag_of_words)\n",
    "\n",
    "print(norm_bag_of_words.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos usar um algoritmo como nossas decision trees para treinar um classificador e descobrir novos spotteds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time classificador = DecisionTreeClassifier().fit(norm_bag_of_words, df['aprovado'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E testamos nosso classificador com dados novos, primeiro vetorizando e normalizando eles, claro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testando = [\"Te achei muito bonita!\", \"Vendo festa na terça feira\"]\n",
    "testando = tfidf_transformer.transform(vetorizador_fitado.transform(testando))\n",
    "\n",
    "print(classificador.predict(testando))\n",
    "\n",
    "# Lembrando que 1 é aprovado e 0 é rejeitado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso nós concluimos nosso estudo sobre análise de textos. Vamos agora tocar o mundo de\n",
    "\n",
    "# Aprendizado não supervisionado\n",
    "\n",
    "Existem muitas áreas ligadas ao aprendizado não supervisionado. Talvez a mais comum seja a de agrupamento, ou **clustering**. Nessa área, nós pegamos dados não nomeados e tentamos encontrar estruturas nesses dados.\n",
    "\n",
    "Primeiro vamos analisar um pouco alguns exemplos de clusteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data2(blue, red, test_size=0.2, func=lambda x: x):\n",
    "    \n",
    "    blue_x = np.vectorize(func)(np.random.randint(*blue[0]))\n",
    "    blue_y = np.vectorize(func)(np.random.randint(*blue[1]))\n",
    "    red_x = np.vectorize(func)(np.random.randint(*red[0]))\n",
    "    red_y = np.vectorize(func)(np.random.randint(*red[1]))\n",
    "    \n",
    "    blue_vals = np.dstack((blue_x, blue_y, [\"blue\" for _ in blue_x]))[0]\n",
    "    red_vals = np.dstack((red_x, red_y, [\"blue\" for _ in red_x]))[0]\n",
    "    \n",
    "    X = np.append(blue_vals, red_vals, axis=0)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = generate_data2([[-60, -20, 30], [0, 40, 30]], [[20, 60, 30], [0, 40, 30]])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=X[:, 2])\n",
    "plt.xlim(-100, 100)\n",
    "plt.ylim(-100, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você definiria a estrutura desse banco de dados?\n",
    "\n",
    "K-Means poderia separar isso de várias formas diferentes\n",
    "\n",
    "Para melhor visualizar como os centros dos clusteres poderiam ser encontrados, vamos usar uma ferramenta diferente:\n",
    "\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "Depois de brincar com esses dados, vamos experimentar usar o KMeans\n",
    "\n",
    "Como tudo no SKLearn, começamos fitando nossos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = KMeans()\n",
    "\n",
    "cluster.fit(X[:, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos pegar os centros que ele encontrou e vamos ver no gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=X[:, 2])\n",
    "plt.xlim(-100, 100)\n",
    "plt.ylim(-100, 100)\n",
    "plt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], color=\"red\", s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ué? Por que tantos centros assim?\n",
    "\n",
    "Existe um parâmetro muito importante em KMeans que é o número de clusteres que devemos encontrar. Vamos alterar esse valor para 2 e ver como ele se sai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=2)\n",
    "\n",
    "cluster.fit(X[:, :2])\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=X[:, 2])\n",
    "plt.xlim(-100, 100)\n",
    "plt.ylim(-100, 100)\n",
    "plt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], color=\"red\", s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora parece muito melhor!\n",
    "\n",
    "Vamos fazer algumas previsões agora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cluster.predict([[10, 10], [-10, -10], [0, 0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bem simples, não?\n",
    "\n",
    "Com esses clusteres novos nós seríamos capazes de até criar labels novas para nossos dados, alimentá-las a Decision Trees e criar classificadores ainda mais poderosos usando uma combinação de aprendizado supervisionado e não supervisionado.\n",
    "\n",
    "Com essas ferramentas você já deve ser capaz de começar a descobrir e prever estruturas em dados usando o poder de Machine Learning. \n",
    "\n",
    "Porém, não pare por aqui. Existe muito mais coisa em machine learning além dessas coisas. Para ser um bom *machine learner* você precisa estudar bastante. É um mercado novo e ainda faltam profissionais atuando, contudo, os que atuam não estão para brincadeira ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
